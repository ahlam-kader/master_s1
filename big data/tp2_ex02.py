from pyspark.sql import SparkSession
from pyspark.sql.functions import col, substring, count, avg, sum

# CrÃ©ation de la session Spark
spark = SparkSession.builder.master("local").appName("Analyse des Accidents de la Route").getOrCreate()

# Chargement du fichier CSV contenant les donnÃ©es
accidents_df = spark.read.option("header", True).option("inferSchema", True).csv("Global_Traffic_Accidents.csv")

# VÃ©rification du schÃ©ma des donnÃ©es
accidents_df.printSchema()

# âœ… Nombre total d'accidents par pays (DataFrame)
accidents_par_pays = accidents_df.groupBy("Country").count().withColumnRenamed("count", "Total_Accidents")
print("ğŸ“Š Nombre d'accidents enregistrÃ©s par pays :")
accidents_par_pays.show()

# âœ… Nombre d'accidents selon leur gravitÃ© (DataFrame)
accidents_par_gravite = accidents_df.groupBy("Severity").count().withColumnRenamed("count", "Nombre_Accidents")
print("âš ï¸ RÃ©partition des accidents en fonction de leur gravitÃ© :")
accidents_par_gravite.show()

# âœ… Moyenne des victimes par type de route (DataFrame)
moyenne_victimes_par_route = accidents_df.groupBy("Road_Type").agg(avg("Number_of_Casualties").alias("Moyenne_Victimes"))
print("ğŸ›£ï¸ Nombre moyen de victimes selon le type de route :")
moyenne_victimes_par_route.show()

# âœ… Heure de la journÃ©e oÃ¹ il y a le plus d'accidents (DataFrame)
accidents_par_heure = accidents_df.withColumn("Heure", substring("Time", 1, 2)) \
    .groupBy("Heure").agg(count("*").alias("Total_Accidents")) \
    .orderBy(col("Total_Accidents").desc())
print("â° Heure oÃ¹ les accidents sont les plus frÃ©quents :")
accidents_par_heure.show(1)

# âœ… Nombre d'accidents lors de mauvaises conditions mÃ©tÃ©orologiques (DataFrame)
conditions_meteo_defavorables = ["Snowy", "Foggy", "Rainy", "Stormy"]
impact_meteo = accidents_df.filter(col("Weather_Condition").isin(conditions_meteo_defavorables)) \
    .groupBy("Weather_Condition").agg(count("*").alias("Nombre_Accidents"))
print("ğŸŒ§ï¸ Nombre d'accidents en fonction des conditions mÃ©tÃ©orologiques difficiles :")
impact_meteo.show()

# âœ… Les 5 villes oÃ¹ il y a eu le plus de victimes (DataFrame)
villes_plus_victimes = accidents_df.groupBy("City").agg(sum("Number_of_Casualties").alias("Total_Victimes")) \
    .orderBy(col("Total_Victimes").desc()).limit(5)
print("ğŸ™ï¸ Top 5 des villes avec le plus grand nombre de victimes :")
villes_plus_victimes.show()

# ğŸ”„ Conversion du DataFrame en RDD
accidents_rdd = accidents_df.rdd

# âœ… Nombre total d'accidents par pays (RDD)
accidents_par_pays_rdd = accidents_rdd.map(lambda row: (row["Country"], 1)) \
                                     .reduceByKey(lambda a, b: a + b)

print("Nombre total d'accidents par pays (via RDD) :")
for pays, total in accidents_par_pays_rdd.collect():
    print(f"{pays}: {total}")

# âœ… Moyenne des victimes par type de route (RDD)
victimes_par_route_rdd = accidents_rdd.map(lambda row: (row["Road_Type"], (row["Number_of_Casualties"], 1))) \
                                      .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])) \
                                      .mapValues(lambda x: x[0] / x[1])  # Calcul de la moyenne

print("Nombre moyen de victimes par type de route (via RDD) :")
for route, moyenne in victimes_par_route_rdd.collect():
    print(f"{route}: {moyenne:.2f}")

# Fermeture de la session Spark
spark.stop()
